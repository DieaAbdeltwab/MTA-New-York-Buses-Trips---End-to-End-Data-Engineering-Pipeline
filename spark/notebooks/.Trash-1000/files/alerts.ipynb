{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "464ae7e6-5207-4a15-aa77-10a8130da66f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import from_json, col, expr, explode\n",
    "from pyspark.sql.types import *\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "df553084-8538-4e66-b09f-055e91c0bdb8",
   "metadata": {},
   "outputs": [],
   "source": [
    "alert_schema = StructType([\n",
    "    StructField(\"header\", StructType([\n",
    "        StructField(\"gtfsRealtimeVersion\", StringType(), True),\n",
    "        StructField(\"timestamp\", StringType(), True)\n",
    "    ]), True),\n",
    "    StructField(\"entity\", ArrayType(StructType([\n",
    "        StructField(\"id\", StringType(), True),\n",
    "        StructField(\"alert\", StructType([\n",
    "            StructField(\"activePeriod\", ArrayType(StructType([\n",
    "                StructField(\"start\", StringType(), True),\n",
    "                StructField(\"end\", StringType(), True)\n",
    "            ]), True), True),\n",
    "            StructField(\"informedEntity\", ArrayType(StructType([\n",
    "                StructField(\"agencyId\", StringType(), True),\n",
    "                StructField(\"routeId\", StringType(), True)\n",
    "            ]), True), True),\n",
    "            StructField(\"headerText\", StructType([\n",
    "                StructField(\"translation\", ArrayType(StructType([\n",
    "                    StructField(\"text\", StringType(), True),\n",
    "                    StructField(\"language\", StringType(), True)\n",
    "                ]), True), True)\n",
    "            ]), True),\n",
    "            StructField(\"descriptionText\", StructType([\n",
    "                StructField(\"translation\", ArrayType(StructType([\n",
    "                    StructField(\"text\", StringType(), True),\n",
    "                    StructField(\"language\", StringType(), True)\n",
    "                ]), True), True)\n",
    "            ]), True)\n",
    "        ]), True)\n",
    "    ]), True), True)\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "12af430f-c15f-4d76-9ec2-92966e6353b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark = SparkSession.builder \\\n",
    "    .appName(\"KafkaConsumerGTFSAlerts\") \\\n",
    "    .master(\"local[*]\") \\\n",
    "    .getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e96a11ec-a897-435a-b2fa-9a20087bb622",
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_df = spark.readStream.format(\"kafka\") \\\n",
    "    .option(\"kafka.bootstrap.servers\", \"broker:29092\") \\\n",
    "    .option(\"subscribe\", \"gtfs-alerts\") \\\n",
    "    .option(\"startingOffsets\", \"earliest\") \\\n",
    "    .load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "433adee4-2e5d-4335-84e9-1e94fa1fb877",
   "metadata": {},
   "outputs": [],
   "source": [
    "kafka_df = raw_df.selectExpr(\"CAST(value AS STRING) AS json_str\")\n",
    "\n",
    "alert_df = kafka_df.select(from_json(col(\"json_str\"), alert_schema).alias(\"data\")) \\\n",
    "    .select(\"data.*\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "02e8ea99-a45c-4ec5-97ba-eec324a12572",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_entity = alert_df.select(\"header\", explode(\"entity\").alias(\"entity\"))\n",
    "\n",
    "df_active_period = df_entity.select(\n",
    "    col(\"header.gtfsRealtimeVersion\").alias(\"gtfsRealtimeVersion\"),\n",
    "    col(\"header.timestamp\").alias(\"timestamp\"),\n",
    "    col(\"entity.id\").alias(\"id\"),\n",
    "    explode(\"entity.alert.activePeriod\").alias(\"activePeriod\"),\n",
    "    col(\"entity.alert.informedEntity\").alias(\"informedEntity\"),\n",
    "    col(\"entity.alert.headerText\").alias(\"headerText\"),\n",
    "    col(\"entity.alert.descriptionText\").alias(\"descriptionText\")\n",
    ").filter(col(\"activePeriod\").isNotNull())\n",
    "\n",
    "df_informed_entity = df_active_period.select(\n",
    "    \"gtfsRealtimeVersion\",\n",
    "    \"timestamp\",\n",
    "    \"id\",\n",
    "    col(\"activePeriod.start\").alias(\"activePeriod_start\"),\n",
    "    col(\"activePeriod.end\").alias(\"activePeriod_end\"),\n",
    "    explode(\"informedEntity\").alias(\"informedEntity\"),\n",
    "    \"headerText\",\n",
    "    \"descriptionText\"\n",
    ").filter(col(\"informedEntity\").isNotNull())\n",
    "\n",
    "df_header_text = df_informed_entity.select(\n",
    "    \"gtfsRealtimeVersion\",\n",
    "    \"timestamp\",\n",
    "    \"id\",\n",
    "    \"activePeriod_start\",\n",
    "    \"activePeriod_end\",\n",
    "    col(\"informedEntity.agencyId\").alias(\"agencyId\"),\n",
    "    col(\"informedEntity.routeId\").alias(\"routeId\"),\n",
    "    explode(\"headerText.translation\").alias(\"headerTranslation\"),\n",
    "    \"descriptionText\"\n",
    ").filter(col(\"headerTranslation\").isNotNull())\n",
    "\n",
    "df_final = df_header_text.select(\n",
    "    \"gtfsRealtimeVersion\",\n",
    "    \"timestamp\",\n",
    "    \"id\",\n",
    "    \"activePeriod_start\",\n",
    "    \"activePeriod_end\",\n",
    "    \"agencyId\",\n",
    "    \"routeId\",\n",
    "    col(\"headerTranslation.text\").alias(\"header_text\"),\n",
    "    col(\"headerTranslation.language\").alias(\"header_language\"),\n",
    "    explode(\"descriptionText.translation\").alias(\"descriptionTranslation\")\n",
    ").filter(col(\"descriptionTranslation\").isNotNull())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "24101de6-313e-4584-890d-6a25b28545c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_final = df_final.select(\n",
    "    \"gtfsRealtimeVersion\",\n",
    "    \"timestamp\",\n",
    "    \"id\",\n",
    "    \"activePeriod_start\",\n",
    "    \"activePeriod_end\",\n",
    "    \"agencyId\",\n",
    "    \"routeId\",\n",
    "    \"header_text\",\n",
    "    \"header_language\",\n",
    "    col(\"descriptionTranslation.text\").alias(\"description_text\"),\n",
    "    col(\"descriptionTranslation.language\").alias(\"description_language\")\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aba9add3-810f-4d65-aa08-63849511679b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def write_to_clickhouse(batch_df, batch_id):\n",
    "    batch_df.write \\\n",
    "        .format(\"jdbc\") \\\n",
    "        .option(\"url\", \"jdbc:clickhouse://clickhouse:8123/gtfs_streaming\") \\\n",
    "        .option(\"dbtable\", \"alerts\") \\\n",
    "        .option(\"user\", \"default\") \\\n",
    "        .option(\"password\", \"123\") \\\n",
    "        .option(\"driver\", \"com.clickhouse.jdbc.ClickHouseDriver\") \\\n",
    "        .mode(\"append\") \\\n",
    "        .save()\n",
    "\n",
    "alert_query = df_final.writeStream \\\n",
    "    .outputMode(\"append\") \\\n",
    "    .foreachBatch(write_to_clickhouse) \\\n",
    "    .option(\"checkpointLocation\", \"/tmp/cp_gtfs_alerts\") \\\n",
    "    .trigger(processingTime=\"30 seconds\") \\\n",
    "    .start()\n",
    "\n",
    "# alert_query.awaitTermination()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "246a987f-c094-480d-b68a-75c1372fef17",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33mWARNING: The directory '/.cache/pip' or its parent directory is not owned or is not writable by the current user. The cache has been disabled. Check the permissions and owner of that directory. If executing pip with sudo, you should use sudo's -H flag.\u001b[0m\u001b[33m\n",
      "\u001b[0mCollecting clickhouse-connect\n",
      "  Downloading clickhouse_connect-0.8.18-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (3.4 kB)\n",
      "Collecting certifi (from clickhouse-connect)\n",
      "  Downloading certifi-2025.8.3-py3-none-any.whl.metadata (2.4 kB)\n",
      "Collecting urllib3>=1.26 (from clickhouse-connect)\n",
      "  Downloading urllib3-2.5.0-py3-none-any.whl.metadata (6.5 kB)\n",
      "Collecting pytz (from clickhouse-connect)\n",
      "  Downloading pytz-2025.2-py2.py3-none-any.whl.metadata (22 kB)\n",
      "Collecting zstandard (from clickhouse-connect)\n",
      "  Downloading zstandard-0.23.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (3.0 kB)\n",
      "Collecting lz4 (from clickhouse-connect)\n",
      "  Downloading lz4-4.4.4-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (3.8 kB)\n",
      "Downloading clickhouse_connect-0.8.18-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.1 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.1/1.1 MB\u001b[0m \u001b[31m3.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m0m\n",
      "\u001b[?25hDownloading urllib3-2.5.0-py3-none-any.whl (129 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m129.8/129.8 kB\u001b[0m \u001b[31m229.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading certifi-2025.8.3-py3-none-any.whl (161 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m161.2/161.2 kB\u001b[0m \u001b[31m65.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading lz4-4.4.4-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.3 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.3/1.3 MB\u001b[0m \u001b[31m15.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading pytz-2025.2-py2.py3-none-any.whl (509 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m509.2/509.2 kB\u001b[0m \u001b[31m145.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading zstandard-0.23.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (5.4 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.4/5.4 MB\u001b[0m \u001b[31m49.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25h\u001b[33mDEPRECATION: Loading egg at /opt/bitnami/python/lib/python3.11/site-packages/pip-23.3.2-py3.11.egg is deprecated. pip 24.3 will enforce this behaviour change. A possible replacement is to use pip for package installation.. Discussion can be found at https://github.com/pypa/pip/issues/12330\u001b[0m\u001b[33m\n",
      "\u001b[0mInstalling collected packages: pytz, zstandard, urllib3, lz4, certifi, clickhouse-connect\n",
      "Successfully installed certifi-2025.8.3 clickhouse-connect-0.8.18 lz4-4.4.4 pytz-2025.2 urllib3-2.5.0 zstandard-0.23.0\n"
     ]
    }
   ],
   "source": [
    "# Pick a writable directory for packages\n",
    "!mkdir -p /opt/notebooks/.python_packages\n",
    "\n",
    "# Install clickhouse-connect into it\n",
    "!pip install --target=/opt/notebooks/.python_packages clickhouse-connect\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "c2c6e8d1-dc6f-4ebd-b03e-bd13356b8dcf",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append(\"/opt/notebooks/.python_packages\")\n",
    "\n",
    "import clickhouse_connect"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6fd13818-3a5e-4952-9924-c558b9bc9bce",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!/usr/bin/env python\n",
    "# coding: utf-8\n",
    "\n",
    "import clickhouse_connect\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import from_json, col, explode\n",
    "from pyspark.sql.types import *\n",
    "\n",
    "# ClickHouse connection details\n",
    "CLICKHOUSE_HOST = \"clickhouse\"\n",
    "CLICKHOUSE_PORT = 8123\n",
    "CLICKHOUSE_USER = \"default\"\n",
    "CLICKHOUSE_PASS = \"123\"\n",
    "CLICKHOUSE_DB = \"gtfs_streaming\"\n",
    "CLICKHOUSE_TABLE = \"gtfs_alerts\"\n",
    "\n",
    "# Create ClickHouse table if it does not exist\n",
    "client = clickhouse_connect.get_client(\n",
    "    host=CLICKHOUSE_HOST,\n",
    "    port=CLICKHOUSE_PORT,\n",
    "    username=CLICKHOUSE_USER,\n",
    "    password=CLICKHOUSE_PASS\n",
    ")\n",
    "\n",
    "# client.command(f\"\"\"\n",
    "# CREATE DATABASE IF NOT EXISTS {CLICKHOUSE_DB}\n",
    "# \"\"\")\n",
    "\n",
    "# client.command(f\"\"\"\n",
    "# CREATE TABLE IF NOT EXISTS {CLICKHOUSE_DB}.{CLICKHOUSE_TABLE} (\n",
    "#     gtfsRealtimeVersion String,\n",
    "#     timestamp String,\n",
    "#     id String,\n",
    "#     activePeriod_start String,\n",
    "#     activePeriod_end String,\n",
    "#     agencyId String,\n",
    "#     routeId String,\n",
    "#     header_text String,\n",
    "#     header_language String,\n",
    "#     description_text String,\n",
    "#     description_language String\n",
    "# ) ENGINE = MergeTree()\n",
    "# ORDER BY id\n",
    "# \"\"\")\n",
    "\n",
    "# print(f\"✅ ClickHouse table `{CLICKHOUSE_DB}.{CLICKHOUSE_TABLE}` is ready.\")\n",
    "\n",
    "# Spark session\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"KafkaConsumerGTFSVP\") \\\n",
    "    .master(\"local[*]\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "# Define schema for GTFS Realtime JSON\n",
    "schema = StructType([\n",
    "    StructField(\"header\", StructType([\n",
    "        StructField(\"gtfsRealtimeVersion\", StringType(), True),\n",
    "        StructField(\"timestamp\", StringType(), True)\n",
    "    ]), True),\n",
    "    StructField(\"entity\", ArrayType(StructType([\n",
    "        StructField(\"id\", StringType(), True),\n",
    "        StructField(\"alert\", StructType([\n",
    "            StructField(\"activePeriod\", ArrayType(StructType([\n",
    "                StructField(\"start\", StringType(), True),\n",
    "                StructField(\"end\", StringType(), True)\n",
    "            ]), True), True),\n",
    "            StructField(\"informedEntity\", ArrayType(StructType([\n",
    "                StructField(\"agencyId\", StringType(), True),\n",
    "                StructField(\"routeId\", StringType(), True)\n",
    "            ]), True), True),\n",
    "            StructField(\"headerText\", StructType([\n",
    "                StructField(\"translation\", ArrayType(StructType([\n",
    "                    StructField(\"text\", StringType(), True),\n",
    "                    StructField(\"language\", StringType(), True)\n",
    "                ]), True), True)\n",
    "            ]), True),\n",
    "            StructField(\"descriptionText\", StructType([\n",
    "                StructField(\"translation\", ArrayType(StructType([\n",
    "                    StructField(\"text\", StringType(), True),\n",
    "                    StructField(\"language\", StringType(), True)\n",
    "                ]), True), True)\n",
    "            ]), True)\n",
    "        ]), True)\n",
    "    ]), True), True)\n",
    "])\n",
    "\n",
    "# Read from Kafka\n",
    "raw_df = spark.readStream.format(\"kafka\") \\\n",
    "    .option(\"kafka.bootstrap.servers\", \"broker:29092\") \\\n",
    "    .option(\"subscribe\", \"gtfs-alerts\") \\\n",
    "    .option(\"startingOffsets\", \"earliest\") \\\n",
    "    .load()\n",
    "\n",
    "kafka_df = raw_df.selectExpr(\"CAST(value AS STRING) AS json_str\", \"topic\")\n",
    "alert_df = kafka_df.select(from_json(col(\"json_str\"), schema).alias(\"data\")).select(\"data.*\")\n",
    "\n",
    "# Flatten nested JSON\n",
    "df_entity = alert_df.select(\"header\", explode(\"entity\").alias(\"entity\"))\n",
    "\n",
    "df_active_period = df_entity.select(\n",
    "    col(\"header.gtfsRealtimeVersion\").alias(\"gtfsRealtimeVersion\"),\n",
    "    col(\"header.timestamp\").alias(\"timestamp\"),\n",
    "    col(\"entity.id\").alias(\"id\"),\n",
    "    explode(\"entity.alert.activePeriod\").alias(\"activePeriod\"),\n",
    "    col(\"entity.alert.informedEntity\").alias(\"informedEntity\"),\n",
    "    col(\"entity.alert.headerText\").alias(\"headerText\"),\n",
    "    col(\"entity.alert.descriptionText\").alias(\"descriptionText\")\n",
    ").filter(col(\"activePeriod\").isNotNull())\n",
    "\n",
    "df_informed_entity = df_active_period.select(\n",
    "    \"gtfsRealtimeVersion\",\n",
    "    \"timestamp\",\n",
    "    \"id\",\n",
    "    col(\"activePeriod.start\").alias(\"activePeriod_start\"),\n",
    "    col(\"activePeriod.end\").alias(\"activePeriod_end\"),\n",
    "    explode(\"informedEntity\").alias(\"informedEntity\"),\n",
    "    \"headerText\",\n",
    "    \"descriptionText\"\n",
    ").filter(col(\"informedEntity\").isNotNull())\n",
    "\n",
    "df_header_text = df_informed_entity.select(\n",
    "    \"gtfsRealtimeVersion\",\n",
    "    \"timestamp\",\n",
    "    \"id\",\n",
    "    \"activePeriod_start\",\n",
    "    \"activePeriod_end\",\n",
    "    col(\"informedEntity.agencyId\").alias(\"agencyId\"),\n",
    "    col(\"informedEntity.routeId\").alias(\"routeId\"),\n",
    "    explode(\"headerText.translation\").alias(\"headerTranslation\"),\n",
    "    \"descriptionText\"\n",
    ").filter(col(\"headerTranslation\").isNotNull())\n",
    "\n",
    "df_final = df_header_text.select(\n",
    "    \"gtfsRealtimeVersion\",\n",
    "    \"timestamp\",\n",
    "    \"id\",\n",
    "    \"activePeriod_start\",\n",
    "    \"activePeriod_end\",\n",
    "    \"agencyId\",\n",
    "    \"routeId\",\n",
    "    col(\"headerTranslation.text\").alias(\"header_text\"),\n",
    "    col(\"headerTranslation.language\").alias(\"header_language\"),\n",
    "    explode(\"descriptionText.translation\").alias(\"descriptionTranslation\")\n",
    ").filter(col(\"descriptionTranslation\").isNotNull())\n",
    "\n",
    "df_final = df_final.select(\n",
    "    \"gtfsRealtimeVersion\",\n",
    "    \"timestamp\",\n",
    "    \"id\",\n",
    "    \"activePeriod_start\",\n",
    "    \"activePeriod_end\",\n",
    "    \"agencyId\",\n",
    "    \"routeId\",\n",
    "    \"header_text\",\n",
    "    \"header_language\",\n",
    "    col(\"descriptionTranslation.text\").alias(\"description_text\"),\n",
    "    col(\"descriptionTranslation.language\").alias(\"description_language\")\n",
    ")\n",
    "\n",
    "# JDBC details for Spark\n",
    "clickhouse_url = f\"jdbc:clickhouse://{CLICKHOUSE_HOST}:{CLICKHOUSE_PORT}/{CLICKHOUSE_DB}\"\n",
    "clickhouse_properties = {\n",
    "    \"user\": CLICKHOUSE_USER,\n",
    "    \"password\": CLICKHOUSE_PASS,\n",
    "    \"driver\": \"com.clickhouse.jdbc.ClickHouseDriver\"\n",
    "}\n",
    "\n",
    "# Function to write each micro-batch to ClickHouse\n",
    "def write_to_clickhouse(batch_df, batch_id):\n",
    "    batch_df.write \\\n",
    "        .mode(\"append\") \\\n",
    "        .jdbc(clickhouse_url, CLICKHOUSE_TABLE, properties=clickhouse_properties)\n",
    "\n",
    "# Start streaming to ClickHouse\n",
    "alert_query = df_final.writeStream \\\n",
    "    .outputMode(\"append\") \\\n",
    "    .foreachBatch(write_to_clickhouse) \\\n",
    "    .trigger(processingTime=\"30 seconds\") \\\n",
    "    .start()\n",
    "\n",
    "alert_query.awaitTermination()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ee2d0f6-071e-4310-bcd1-55171c091681",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07c7d043-5ab3-47e0-a919-ebc57c159362",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
